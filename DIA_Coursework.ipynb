{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install inferactively-pymdp"
      ],
      "metadata": {
        "id": "9wG9keMzJ5JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqj2MU7WIG_t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from pymdp.agent import Agent\n",
        "from pymdp import utils, maths\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.cm as cm\n",
        "import random\n",
        "import time\n",
        "\n",
        "#Generate 8 random tuples\n",
        "def generate_unique_tuple(exclude_set):\n",
        "    while True:\n",
        "        x = random.randint(0, 5)\n",
        "        y = random.randint(0, 5)\n",
        "        new_tuple = (x, y)\n",
        "        if new_tuple not in exclude_set:\n",
        "            return new_tuple\n",
        "\n",
        "results = []\n",
        "for loops_iter in range(0,10):\n",
        "  seen_tuples = set()\n",
        "  seen_tuples.add((0,0))\n",
        "  seen_tuples.add((2,0))\n",
        "  seen_tuples.add((1,1))\n",
        "  seen_tuples.add((2,2))\n",
        "  seen_tuples.add((1,0))\n",
        "  seen_tuples.add((0,1))\n",
        "\n",
        "\n",
        "  tuple1 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple1)\n",
        "  tuple2 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple2)\n",
        "  tuple3 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple3)\n",
        "  tuple4 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple4)\n",
        "  tuple5 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple5)\n",
        "  tuple6 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple6)\n",
        "  tuple7 = generate_unique_tuple(seen_tuples)\n",
        "  seen_tuples.add(tuple7)\n",
        "  tuple8 = generate_unique_tuple(seen_tuples)\n",
        "\n",
        "\n",
        "  #Define the world\n",
        "  world_size = [6, 6]\n",
        "  num_grid_points = np.prod(world_size) # total number of grid locations (rows X columns)\n",
        "\n",
        "  # lookup table to convert location (x,y) tuples to index\n",
        "  grid = np.arange(num_grid_points).reshape(world_size)\n",
        "  it = np.nditer(grid, flags=[\"multi_index\"])\n",
        "\n",
        "  grid_location_lookup = []\n",
        "  while not it.finished:\n",
        "      grid_location_lookup.append(it.multi_index)\n",
        "      it.iternext()\n",
        "\n",
        "\n",
        "  # Hidden Random Variables cheatsheat_locations and destination_locations, and names corresponding to their realizations\n",
        "  cheatsheat_names = ['CS1', 'CS2', 'CS3', 'CS4']\n",
        "  cheatsheat_locations = [tuple5, tuple6, tuple7, tuple8]\n",
        "\n",
        "  destination_names = [\"A\", \"B\", \"C\"]\n",
        "  destination_locations = [tuple1, tuple2, tuple3]\n",
        "\n",
        "  hidden_state_combinations = [num_grid_points, len(cheatsheat_locations), len(destination_names)]\n",
        "\n",
        "\n",
        "  #Observable Random Variables\n",
        "  cheatsheat_hint_location = (2, 0)\n",
        "  cheatsheat_hints = ['Nothing'] + cheatsheat_names\n",
        "  destination_hints = ['Nothing', 'A', 'B', 'C']\n",
        "  destination_reward_names = ['Nothing', 'Cheese', 'Tom'] #reward could be nothing, positive or negative\n",
        "\n",
        "  observable_state_combinations = [num_grid_points, len(cheatsheat_hints), len(destination_hints), len(destination_reward_names)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  likelihood_matrices = utils.obj_array_zeros([ [o_dim] + hidden_state_combinations for o_dim in observable_state_combinations])\n",
        "\n",
        "  # likelihoods of location obervation depends only on hidden location\n",
        "  # Hence likelihoods would be 1 given the hidden state's current_location being same as observed_location\n",
        "  likelihood_matrices[0] = np.tile(np.expand_dims(np.eye(num_grid_points), (-2, -1)), (1, 1, hidden_state_combinations[1], hidden_state_combinations[2]))\n",
        "\n",
        "  #likelihoods for cheatsheat_hints observation. 'Nothing' is the most likely observation eevrywhere except spots where cheat sheat hint is located\n",
        "  #likelihood of each realization tied to following hidden variables: current_location and cheatsheat_locations\n",
        "  likelihood_matrices[1][0,:,:,:] = 1.0\n",
        "  for i in range(0, len(cheatsheat_locations)):\n",
        "      likelihood_matrices[1][0, grid_location_lookup.index(cheatsheat_hint_location),i,:] = 0.0\n",
        "      likelihood_matrices[1][i+1, grid_location_lookup.index(cheatsheat_hint_location),i,:] = 1.0 # full likelihood for realization corresponding to cheatsheat_location\n",
        "\n",
        "\n",
        "  #likelihoods for cheatsheat_hints observation. 'Nothing' is the most likely observation everywhere except spots where cheat sheat is located\n",
        "  #likelihood of each realization tied to following hidden variables: current_location and destination_locations\n",
        "  likelihood_matrices[2][0,:,:,:] = 1.0\n",
        "  for i, cheatsheat_loc in enumerate(cheatsheat_locations):\n",
        "    likelihood_matrices[2][0,grid_location_lookup.index(cheatsheat_loc),i,:] = 0.0  # zero likelihood for 'Nothing' realization\n",
        "    for j in range(0, len(destination_names)):\n",
        "      likelihood_matrices[2][j+1,grid_location_lookup.index(cheatsheat_loc),i,j] = 1.0 # full likelihood for realization corresponding to destination_location\n",
        "\n",
        "  #Reward Likelihoods\n",
        "  likelihood_matrices[3][0,:,:,:] = 1.0\n",
        "  i = 0\n",
        "  for destination_loc in destination_locations:\n",
        "    loc_index = grid_location_lookup.index(destination_loc)\n",
        "    likelihood_matrices[3][0,loc_index,:,:] = 0.0\n",
        "    for j in range(0,len(destination_names)):\n",
        "      if j == i:\n",
        "        likelihood_matrices[3][1,loc_index,:,j] = 1.0\n",
        "      else:\n",
        "        likelihood_matrices[3][2,loc_index,:,j] = 1.0\n",
        "    i +=1\n",
        "\n",
        "\n",
        "  # Prior probability distribution for each hidden random variable will be uniform, except for grid location\n",
        "  initial_prior_distribution = utils.obj_array_uniform(hidden_state_combinations)\n",
        "  initial_prior_distribution[0] = utils.onehot(grid_location_lookup.index((0,0)), num_grid_points)\n",
        "\n",
        "  # agent's preference for all states would be zero except for the state corresponding to cheese reward. For state corresponding to tom reward, there's negative preference\n",
        "  state_preferences = utils.obj_array_zeros(observable_state_combinations)\n",
        "  state_preferences[3][1] = 2.0 # make the agent want to encounter the \"Cheese\" observation level\n",
        "  state_preferences[3][2] = -4.0 # make the agent not want to encounter the \"Tom\" observation level\n",
        "\n",
        "\n",
        "  #B would be the transition probabilities matrix\n",
        "  num_controls = [5, 1, 1]\n",
        "  B_f_shapes = [ [ns, ns, num_controls[f]] for f, ns in enumerate(hidden_state_combinations)]\n",
        "  B = utils.obj_array_zeros(B_f_shapes)\n",
        "  actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"STAY\"]\n",
        "  for action_id, action_label in enumerate(actions):\n",
        "\n",
        "    for curr_state, grid_location in enumerate(grid_location_lookup):\n",
        "\n",
        "      y, x = grid_location\n",
        "\n",
        "      if action_label == \"UP\":\n",
        "        next_y = y - 1 if y > 0 else y\n",
        "        next_x = x\n",
        "      elif action_label == \"DOWN\":\n",
        "        next_y = y + 1 if y < (world_size[0]-1) else y\n",
        "        next_x = x\n",
        "      elif action_label == \"LEFT\":\n",
        "        next_x = x - 1 if x > 0 else x\n",
        "        next_y = y\n",
        "      elif action_label == \"RIGHT\":\n",
        "        next_x = x + 1 if x < (world_size[1]-1) else x\n",
        "        next_y = y\n",
        "      elif action_label == \"STAY\":\n",
        "        next_x = x\n",
        "        next_y = y\n",
        "\n",
        "      new_location = (next_y, next_x)\n",
        "      next_state = grid_location_lookup.index(new_location)\n",
        "      B[0][next_state, curr_state, action_id] = 1.0\n",
        "  B[1][:,:,0] = np.eye(hidden_state_combinations[1])\n",
        "  B[2][:,:,0] = np.eye(hidden_state_combinations[2])\n",
        "\n",
        "\n",
        "\n",
        "  #This class defines the environment in which the agent will operate.\n",
        "  #Step method is called when the agent performs an action. The method modifies the observable state which the agent would see at next time step\n",
        "  #Reset method just resets the observations to what they were at time step 0\n",
        "  class Environment():\n",
        "      def __init__(self,starting_loc = (0,0), cheatsheat_hint_location = (2, 0), cheatsheat_name = 'CS1', destination_name = 'A'):\n",
        "\n",
        "          self.starting_location = starting_loc\n",
        "          self.current_location = self.starting_location\n",
        "\n",
        "          self.cheatsheat_hint_location = cheatsheat_hint_location\n",
        "          self.cheatsheat_name = cheatsheat_name\n",
        "          self.cheatsheat_location = cheatsheat_locations[cheatsheat_names.index(self.cheatsheat_name)]\n",
        "          self.destination_name = destination_name\n",
        "          print(f'Starting location is {self.starting_location}, Reward condition is {self.destination_name}, cue is located in {self.cheatsheat_name}')\n",
        "\n",
        "      def step(self,action_label):\n",
        "\n",
        "          (Y, X) = self.current_location\n",
        "\n",
        "          #Update the coordinates\n",
        "          if action_label == \"UP\":\n",
        "\n",
        "            Y_new = Y - 1 if Y > 0 else Y\n",
        "            X_new = X\n",
        "\n",
        "          elif action_label == \"DOWN\":\n",
        "\n",
        "            Y_new = Y + 1 if Y < (world_size[0]-1) else Y\n",
        "            X_new = X\n",
        "\n",
        "          elif action_label == \"LEFT\":\n",
        "            Y_new = Y\n",
        "            X_new = X - 1 if X > 0 else X\n",
        "\n",
        "          elif action_label == \"RIGHT\":\n",
        "            Y_new = Y\n",
        "            X_new = X +1 if X < (world_size[1]-1) else X\n",
        "\n",
        "          elif action_label == \"STAY\":\n",
        "            Y_new, X_new = Y, X\n",
        "\n",
        "          self.current_location = (Y_new, X_new) # store the new grid location\n",
        "\n",
        "\n",
        "          #Update the observations\n",
        "          location_observation = self.current_location\n",
        "\n",
        "          #reveal the hint of cheat sheat location when jerry reaches cheatsheat_hint_location\n",
        "          if self.current_location == self.cheatsheat_hint_location:\n",
        "            cheatsheat_hint_observation = self.cheatsheat_name\n",
        "          else:\n",
        "            cheatsheat_hint_observation = 'Nothing'\n",
        "\n",
        "          if self.current_location == self.cheatsheat_location:\n",
        "            destination_hint_observation = destination_hints[destination_names.index(self.destination_name)+1]\n",
        "          else:\n",
        "            destination_hint_observation = 'Nothing'\n",
        "\n",
        "          #reveal the hint of destination location when jerry reaches cheatsheat_location\n",
        "          if self.current_location == destination_locations[0]:\n",
        "            if self.destination_name == 'A':\n",
        "              destination_reward_name_observation = 'Cheese'\n",
        "            else:\n",
        "              destination_reward_name_observation = 'Tom'\n",
        "          elif self.current_location == destination_locations[1]:\n",
        "            if self.destination_name == 'B':\n",
        "              destination_reward_name_observation = 'Cheese'\n",
        "            else:\n",
        "              destination_reward_name_observation = 'Tom'\n",
        "          elif self.current_location == destination_locations[2]:\n",
        "            if self.destination_name == 'C':\n",
        "              destination_reward_name_observation = 'Cheese'\n",
        "            else:\n",
        "              destination_reward_name_observation = 'Tom'\n",
        "          else:\n",
        "            destination_reward_name_observation = 'Nothing'\n",
        "\n",
        "          return location_observation, cheatsheat_hint_observation, destination_hint_observation, destination_reward_name_observation\n",
        "\n",
        "      def reset(self):\n",
        "          self.current_location = self.starting_location\n",
        "          print(f'Re-initialized location to {self.starting_location}')\n",
        "          location_observation = self.current_location\n",
        "          cheatsheat_hint_observation = 'Nothing'\n",
        "          destination_hint_observation = 'Nothing'\n",
        "          destination_reward_name_observation = 'Nothing'\n",
        "\n",
        "          return location_observation, cheatsheat_hint_observation, destination_hint_observation, destination_reward_name_observation\n",
        "\n",
        "\n",
        "\n",
        "  # I finally instantiate the agent and the environment. Agent takes in its constructor the likelihood_matrices, state_preferences, initial_prior_distribution, and action transition matrix B\n",
        "  fristonian_jerry = Agent(A = likelihood_matrices, B = B, C = state_preferences, D = initial_prior_distribution, policy_len = 4)\n",
        "\n",
        "  #I specify cheatsheat hint location, cheatsheat name and destination name\n",
        "  env = Environment(starting_loc = (0,0), cheatsheat_hint_location = (2, 0), cheatsheat_name = 'CS4', destination_name = 'B')\n",
        "\n",
        "  location_observation, cheatsheat_hint_observation, destination_hint_observation, destination_reward_name_observation = env.reset()\n",
        "\n",
        "\n",
        "  history_of_locs = [location_observation]\n",
        "  obs = [grid_location_lookup.index(location_observation), cheatsheat_hints.index(cheatsheat_hint_observation), destination_hints.index(destination_hint_observation), destination_reward_names.index(destination_reward_name_observation)]\n",
        "\n",
        "  #Active inference loop\n",
        "  T = 12 # number of total timesteps\n",
        "  start_time = time.time()\n",
        "  for t in range(T):\n",
        "\n",
        "      qs = fristonian_jerry.infer_states(obs)\n",
        "\n",
        "      fristonian_jerry.infer_policies()\n",
        "      chosen_action_id = fristonian_jerry.sample_action()\n",
        "\n",
        "      movement_id = int(chosen_action_id[0])\n",
        "\n",
        "      choice_action = actions[movement_id]\n",
        "\n",
        "      location_observation, cheatsheat_hint_observation, destination_hint_observation, destination_reward_name_observation = env.step(choice_action)\n",
        "\n",
        "      obs = [grid_location_lookup.index(location_observation), cheatsheat_hints.index(cheatsheat_hint_observation), destination_hints.index(destination_hint_observation), destination_reward_names.index(destination_reward_name_observation)]\n",
        "\n",
        "      history_of_locs.append(location_observation)\n",
        "\n",
        "\n",
        "      if(destination_reward_name_observation == \"Cheese\"):\n",
        "        break\n",
        "      elif destination_reward_name_observation == \"Tom\":\n",
        "        break\n",
        "\n",
        "  results.append([t, destination_reward_name_observation, time.time()-start_time])\n",
        "  print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times = []\n",
        "steps = []\n",
        "rewards = []\n",
        "\n",
        "cheeses = 0\n",
        "cheese_times_total = 0\n",
        "cheese_steps_total = 0\n",
        "\n",
        "for r in results:\n",
        "  times.append(r[2])\n",
        "  steps.append(r[0])\n",
        "  rewards.append(r[1])\n",
        "  if r[1] == 'Cheese':\n",
        "    cheeses += 1\n",
        "    cheese_times_total += r[2]\n",
        "    cheese_steps_total += r[0]\n",
        "\n",
        "print(cheeses/10)\n",
        "print(sum(times) / 10)\n",
        "print(cheese_steps_total / cheeses)\n",
        "\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame({\"Reward\": rewards, \"Time (s)\": times, \"Steps\": steps})\n",
        "results_df.to_excel(\"results.xlsx\")"
      ],
      "metadata": {
        "id": "2YdWAAGjLESG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(range(1, 11), times, marker='o', linestyle='-', color='orange')\n",
        "ax1.set_xlabel('Iteration')\n",
        "ax1.set_ylabel('Time Taken')\n",
        "ax1.set_title('Time Taken (s) during each iteration')\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(range(1, 11), steps, marker='o', linestyle='-', color='blue')\n",
        "ax2.set_xlabel('Iteration')\n",
        "ax2.set_ylabel('Steps Taken')\n",
        "ax2.set_title('Steps Taken during each iteration')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Is6S8u15LHFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization using pygame\n",
        "# set loops_iter range to 1 before running\n",
        "\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "destination_location = destination_locations[destination_names.index('B')]\n",
        "cheatsheat_location = cheatsheat_locations[cheatsheat_names.index('CS4')]\n",
        "\n",
        "BLACK = (0, 0, 0)\n",
        "WHITE = (255, 255, 255)\n",
        "RED = (255, 0, 0)\n",
        "GRAY = (200, 200, 200)\n",
        "\n",
        "pygame.init()\n",
        "\n",
        "window_size = (800, 600)\n",
        "screen = pygame.Surface(window_size)\n",
        "screen.fill(WHITE)\n",
        "\n",
        "\n",
        "cell_width = window_size[0] // (world_size[0])\n",
        "cell_height = window_size[1] // (world_size[1])\n",
        "for row in range(world_size[0]):\n",
        "    for col in range(world_size[1]):\n",
        "        pygame.draw.rect(screen, GRAY, (col * cell_width, row * cell_height, cell_width, cell_height), 1)\n",
        "\n",
        "pygame.draw.rect(screen, GRAY, (0 * cell_width, 2 * cell_height, cell_width, cell_height))\n",
        "\n",
        "tom_img = pygame.image.load(\"/content/Tom.webp\")\n",
        "tom_img = pygame.transform.scale(tom_img, (cell_width, cell_height))\n",
        "\n",
        "jerry_img = pygame.image.load(\"/content/jerry.png\")\n",
        "jerry_img = pygame.transform.scale(jerry_img, (cell_width, cell_height))\n",
        "\n",
        "cheatsheat_img = pygame.image.load(\"/content/mapicon.jpeg\")\n",
        "cheatsheat_img = pygame.transform.scale(cheatsheat_img, (cell_width, cell_height))\n",
        "\n",
        "fake_cheatsheat_img = pygame.image.load(\"/content/mapicon_blurred.png\")\n",
        "fake_cheatsheat_img = pygame.transform.scale(fake_cheatsheat_img, (cell_width, cell_height))\n",
        "\n",
        "cheese_img = pygame.image.load(\"/content/cheese.webp\")\n",
        "cheese_img = pygame.transform.scale(cheese_img, (cell_width, cell_height))\n",
        "\n",
        "screen.blit(jerry_img, (0 * cell_width, 0 * cell_height))\n",
        "\n",
        "for d in destination_locations:\n",
        "  if d == destination_location:\n",
        "    screen.blit(cheese_img, (d[1] * cell_width, d[0] * cell_height))\n",
        "  else:\n",
        "    screen.blit(tom_img, (d[1] * cell_width, d[0] * cell_height))\n",
        "\n",
        "for c in cheatsheat_locations:\n",
        "  if c == cheatsheat_location:\n",
        "    screen.blit(cheatsheat_img, (c[1] * cell_width, c[0] * cell_height))\n",
        "  else:\n",
        "    screen.blit(fake_cheatsheat_img, (c[1] * cell_width, c[0] * cell_height))\n",
        "\n",
        "# jerry's path\n",
        "all_locations = np.vstack(history_of_locs).astype(float)\n",
        "if len(all_locations) > 1:\n",
        "    pygame.draw.lines(screen, RED, False, [(int(loc[1]) * cell_width + cell_width // 2, int(loc[0]) * cell_height + cell_height // 2) for loc in all_locations], 2)\n",
        "\n",
        "\n",
        "image_data = pygame.surfarray.array3d(screen)\n",
        "image_data = np.moveaxis(image_data, 0, 1)\n",
        "pygame.image.save(screen, 'grid_visualization.png')\n",
        "\n",
        "pygame.quit()\n"
      ],
      "metadata": {
        "id": "3rGHkzhtLO4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}